Here's a full working pipeline to:

Convert a PyTorch/ONNX model ‚Üí Optimize using TensorRT ‚Üí Deploy on Jetson with Python inferencing.

‚öôÔ∏è 1. Install TensorRT (Jetson)
If you're on a Jetson device (e.g., Nano/Xavier NX):

bash
Copy
Edit
sudo apt update
sudo apt install python3-libnvinfer libnvinfer-dev libnvinfer-plugin-dev
üîÅ 2. Convert PyTorch ‚Üí ONNX ‚Üí TensorRT
Step A: PyTorch ‚Üí ONNX
python
Copy
Edit
import torch
import torchvision.models as models

model = models.resnet18(pretrained=True)
model.eval()

dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(model, dummy_input, "resnet18.onnx", verbose=False)
Step B: ONNX ‚Üí TensorRT
bash
Copy
Edit
# Requires `onnx-tensorrt` tool (install with pip or from source)
trtexec --onnx=resnet18.onnx --saveEngine=resnet18.trt --fp16
üß† 3. Run Inference with TensorRT in Python
python
Copy
Edit
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
import cv2

TRT_LOGGER = trt.Logger(trt.Logger.INFO)

# Load the engine
with open("resnet18.trt", "rb") as f, trt.Runtime(TRT_LOGGER) as runtime:
    engine = runtime.deserialize_cuda_engine(f.read())

context = engine.create_execution_context()

# Allocate buffers
input_shape = (1, 3, 224, 224)
output_shape = (1, 1000)
d_input = cuda.mem_alloc(np.prod(input_shape) * np.float32().nbytes)
d_output = cuda.mem_alloc(np.prod(output_shape) * np.float32().nbytes)

stream = cuda.Stream()

# Load image & preprocess
img = cv2.imread("dog.jpg")
img = cv2.resize(img, (224, 224)).astype(np.float32) / 255
img = img.transpose((2, 0, 1)).reshape(input_shape)

# Inference
cuda.memcpy_htod_async(d_input, img, stream)
context.execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)
output = np.empty(output_shape, dtype=np.float32)
cuda.memcpy_dtoh_async(output, d_output, stream)
stream.synchronize()

print("Predictions:", output)
üìå TensorRT Optimization Flags:
--fp16: Convert weights and activations to FP16 (faster, smaller)

--int8: Use INT8 precision (needs calibration)

--workspace: Set max memory size (e.g., --workspace=4096)

üîç Pro Tip:
To visualize inference speed:

bash
Copy
Edit
trtexec --onnx=model.onnx --fp16 --verbose --dumpProfile





# ‚ö° TensorRT INT8 Inference Pipeline on Jetson Xavier NX
# Includes: Calibration + Streaming Inference with DeepStream Compatibility

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
import cv2
import os

TRT_LOGGER = trt.Logger(trt.Logger.INFO)

class ModelCalibrator(trt.IInt8EntropyCalibrator2):
    def __init__(self, calibration_files, batch_size, input_shape):
        super(ModelCalibrator, self).__init__()
        self.batch_size = batch_size
        self.input_shape = input_shape
        self.files = calibration_files
        self.index = 0
        self.device_input = cuda.mem_alloc(trt.volume(input_shape) * batch_size * np.float32().nbytes)

    def get_batch_size(self):
        return self.batch_size

    def get_batch(self, names):
        if self.index + self.batch_size > len(self.files):
            return None
        batch = []
        for i in range(self.batch_size):
            img = cv2.imread(self.files[self.index + i])
            img = cv2.resize(img, (self.input_shape[2], self.input_shape[1]))
            img = img.transpose((2, 0, 1)).astype(np.float32).ravel()
            batch.append(img)
        self.index += self.batch_size
        batch = np.ascontiguousarray(batch).ravel()
        cuda.memcpy_htod(self.device_input, batch)
        return [int(self.device_input)]

    def read_calibration_cache(self):
        return None

    def write_calibration_cache(self, cache):
        with open("calibration.cache", "wb") as f:
            f.write(cache)

# üîÅ Engine Build Function for INT8

def build_engine_onnx_int8(onnx_file_path, calibration_files):
    builder = trt.Builder(TRT_LOGGER)
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30
    config.set_flag(trt.BuilderFlag.INT8)

    profile = builder.create_optimization_profile()
    network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    network = builder.create_network(network_flags)

    parser = trt.OnnxParser(network, TRT_LOGGER)
    with open(onnx_file_path, 'rb') as model:
        parser.parse(model.read())

    input_shape = network.get_input(0).shape
    profile.set_shape(network.get_input(0).name, tuple(input_shape), tuple(input_shape), tuple(input_shape))
    config.add_optimization_profile(profile)

    calibrator = ModelCalibrator(calibration_files, batch_size=4, input_shape=input_shape)
    config.int8_calibrator = calibrator

    engine = builder.build_engine(network, config)
    with open("model_int8.trt", "wb") as f:
        f.write(engine.serialize())
    print("‚úÖ INT8 Engine Created: model_int8.trt")

# üì¶ Run INT8 Conversion

onnx_path = "resnet18.onnx"
calibration_imgs = [os.path.join("calib", f) for f in os.listdir("calib") if f.endswith(".jpg")]
build_engine_onnx_int8(onnx_path, calibration_imgs)

# üß† Xavier Stream Optimization Complete


# ‚ö° TensorRT INT8 Inference Pipeline on Jetson Xavier NX with ROS2 Node Support
# Includes: Calibration + Streaming Inference + ROS2 Publisher Node

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
import cv2
import os
import rclpy
from rclpy.node import Node
from std_msgs.msg import String

TRT_LOGGER = trt.Logger(trt.Logger.INFO)

class ModelCalibrator(trt.IInt8EntropyCalibrator2):
    def __init__(self, calibration_files, batch_size, input_shape):
        super(ModelCalibrator, self).__init__()
        self.batch_size = batch_size
        self.input_shape = input_shape
        self.files = calibration_files
        self.index = 0
        self.device_input = cuda.mem_alloc(trt.volume(input_shape) * batch_size * np.float32().nbytes)

    def get_batch_size(self):
        return self.batch_size

    def get_batch(self, names):
        if self.index + self.batch_size > len(self.files):
            return None
        batch = []
        for i in range(self.batch_size):
            img = cv2.imread(self.files[self.index + i])
            img = cv2.resize(img, (self.input_shape[2], self.input_shape[1]))
            img = img.transpose((2, 0, 1)).astype(np.float32).ravel()
            batch.append(img)
        self.index += self.batch_size
        batch = np.ascontiguousarray(batch).ravel()
        cuda.memcpy_htod(self.device_input, batch)
        return [int(self.device_input)]

    def read_calibration_cache(self):
        return None

    def write_calibration_cache(self, cache):
        with open("calibration.cache", "wb") as f:
            f.write(cache)

# üîÅ Engine Build Function for INT8

def build_engine_onnx_int8(onnx_file_path, calibration_files):
    builder = trt.Builder(TRT_LOGGER)
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30
    config.set_flag(trt.BuilderFlag.INT8)

    profile = builder.create_optimization_profile()
    network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    network = builder.create_network(network_flags)

    parser = trt.OnnxParser(network, TRT_LOGGER)
    with open(onnx_file_path, 'rb') as model:
        parser.parse(model.read())

    input_shape = network.get_input(0).shape
    profile.set_shape(network.get_input(0).name, tuple(input_shape), tuple(input_shape), tuple(input_shape))
    config.add_optimization_profile(profile)

    calibrator = ModelCalibrator(calibration_files, batch_size=4, input_shape=input_shape)
    config.int8_calibrator = calibrator

    engine = builder.build_engine(network, config)
    with open("model_int8.trt", "wb") as f:
        f.write(engine.serialize())
    print("‚úÖ INT8 Engine Created: model_int8.trt")

# üì¶ Run INT8 Conversion

onnx_path = "resnet18.onnx"
calibration_imgs = [os.path.join("calib", f) for f in os.listdir("calib") if f.endswith(".jpg")]
build_engine_onnx_int8(onnx_path, calibration_imgs)

# ü§ñ ROS2 Node for inference output
class TRTNode(Node):
    def __init__(self):
        super().__init__('trt_inference_node')
        self.publisher_ = self.create_publisher(String, 'inference_result', 10)
        self.timer = self.create_timer(1.0, self.timer_callback)

    def timer_callback(self):
        # Load engine
        with open("model_int8.trt", "rb") as f, trt.Runtime(TRT_LOGGER) as runtime:
            engine = runtime.deserialize_cuda_engine(f.read())
            context = engine.create_execution_context()

        input_shape = (1, 3, 224, 224)
        output_shape = (1, 1000)
        d_input = cuda.mem_alloc(np.prod(input_shape) * np.float32().nbytes)
        d_output = cuda.mem_alloc(np.prod(output_shape) * np.float32().nbytes)
        stream = cuda.Stream()

        img = cv2.imread("dog.jpg")
        img = cv2.resize(img, (224, 224)).astype(np.float32) / 255
        img = img.transpose((2, 0, 1)).reshape(input_shape)

        cuda.memcpy_htod_async(d_input, img, stream)
        context.execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)
        output = np.empty(output_shape, dtype=np.float32)
        cuda.memcpy_dtoh_async(output, d_output, stream)
        stream.synchronize()

        msg = String()
        msg.data = f"Predictions: {np.argmax(output)}"
        self.publisher_.publish(msg)
        self.get_logger().info(f"üß† Published Inference: {msg.data}")

# Start ROS2 Node
rclpy.init()
node = TRTNode()
rclpy.spin(node)
node.destroy_node()
rclpy.shutdown()

# üß† Deployment with EdgeOps + ROS2 complete



‚úÖ Launch file added to your TensorRT pipeline.

Now you can launch the entire INT8 inference + ROS2 stack via:

bash
ros2 launch your_ros2_package_name trt_infer_launch.py
# ‚ö° TensorRT INT8 Inference Pipeline on Jetson Xavier NX with ROS2 Node Support
# Includes: Calibration + Streaming Inference + ROS2 Publisher Node + ROS2 Launch File

# ROS2 launch file (save this as `trt_infer_launch.py` in your launch/ directory):

from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='your_ros2_package_name',  # Replace with your actual package name
            executable='your_ros2_executable_name',  # Replace with your compiled node executable
            name='trt_inference_node',
            output='screen',
            parameters=[
                {'engine_path': 'model_int8.trt'},
                {'input_image': 'dog.jpg'},
                {'input_shape': [1, 3, 224, 224]},
                {'output_shape': [1, 1000]}
            ]
        )
    ])

# Ensure your `setup.py` and `CMakeLists.txt` have this launch file installed properly.
# Add it under `install(DIRECTORY launch DESTINATION share/<package_name>)`

# üß† ROS2 Launch File Integration Done ‚Äî now just:
# üöÄ ros2 launch your_ros2_package_name trt_infer_launch.py




‚úÖ Your ROS2 package is now fully configured for launch support.

You‚Äôre ready to roll out trt_infer_pkg with:

bash
ros2 launch trt_infer_pkg trt_infer_launch.py
# ‚ö° TensorRT INT8 Inference Pipeline on Jetson Xavier NX with ROS2 Node Support
# Includes: Calibration + Streaming Inference + ROS2 Publisher Node + ROS2 Launch File + ROS2 Package Configuration

# ROS2 launch file (save this as `trt_infer_launch.py` in your launch/ directory):

from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='trt_infer_pkg',  # Replace with your actual package name
            executable='trt_infer_exec',  # Replace with your compiled node executable
            name='trt_inference_node',
            output='screen',
            parameters=[
                {'engine_path': 'model_int8.trt'},
                {'input_image': 'dog.jpg'},
                {'input_shape': [1, 3, 224, 224]},
                {'output_shape': [1, 1000]}
            ]
        )
    ])

# üß© In `setup.py`, ensure the following:
# ----------------------------------------
# from setuptools import setup
# import os
# from glob import glob
#
# package_name = 'trt_infer_pkg'
#
# setup(
#     ...,
#     data_files=[
#         ('share/' + package_name, ['package.xml']),
#         ('share/' + package_name + '/launch', glob('launch/*.py')),
#     ],
#     ...,
# )

# üõ†Ô∏è In `CMakeLists.txt`, include:
# --------------------------------
# install(DIRECTORY launch/
#   DESTINATION share/${PROJECT_NAME}/launch
# )

# üì¶ In `package.xml`, make sure:
# -------------------------------
# <exec_depend>launch</exec_depend>
# <exec_depend>launch_ros</exec_depend>

# üß† ROS2 Launch File + Package Config Complete ‚Äî
# üöÄ Deploy with:
# ros2 launch trt_infer_pkg trt_infer_launch.py
